# κ°μ„ μ‚¬ν•­ λ° μ¶”κ°€ κΈ°λ¥ κ°€μ΄λ“

## π“ qwen_advanced.py vs qwen_vllm_compatible.py

### qwen_advanced.py (κΈ°λ³Έ κµ¬ν„)
- β… Flash Attention, GQA, mHC λ“± ν•µμ‹¬ κΈ°μ 
- β HuggingFace νΈν™ X
- β vLLM μ„λΉ™ μ¤€λΉ„ X
- β Weight initialization λ―Έν΅
- μ©λ„: **ν•™μµ λ° μ—°κµ¬**

### qwen_vllm_compatible.py (ν”„λ΅λ•μ… λ²„μ „)
- β… HuggingFace transformers μ™„λ²½ νΈν™
- β… vLLM μ„λΉ™ μµμ ν™”
- β… PreTrainedModel μƒμ† (ν‘μ¤€ μΈν„°νμ΄μ¤)
- β… Config μ €μ¥/λ΅λ“ JSON ν•μ‹
- β… Generation config μ§€μ›
- β… Xavier weight initialization
- β… Generation ν•¨μ (top-p, top-k μ§€μ›)
- μ©λ„: **μ„λΉ™ λ° λ°°ν¬**

## π€ μ¶”κ°€ κΈ°λ¥λ“¤

### 1. **Distributed Training μ§€μ›**
```python
# DDP (Distributed Data Parallel)
from torch.nn.parallel import DistributedDataParallel as DDP

model = AdvancedQwenForCausalLM(config).to(device)
model = DDP(model, device_ids=[0, 1, 2, 3])
```

### 2. **λ” κ³ κΈ‰ Tokenizer**
```python
# μ‹¤λ¬΄μ—μ„λ” μ΄λ“¤μ„ μ‚¬μ©:
# - SentencePiece (Google)
# - BPE via tokenizers (Hugging Face)
# - Tiktoken (OpenAI)

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B")
```

### 3. **Tensor Parallel (vLLM νΈν™)**
```python
# vLLMμ΄ μλ™μΌλ΅ μ²λ¦¬
# --tensor-parallel-size 4 (4κ° GPU)
```

### 4. **μ–‘μν™” (Q-LoRA)**
```python
from bitsandbytes.nn import Linear4bit

# 4-bit μ–‘μν™”
# λ¨λΈ ν¬κΈ°: 40B β†’ 10GB
```

### 5. **λ” λ‚μ€ Attention κµ¬ν„λ“¤**
- β… Flash Attention v2 (λ” λΉ λ¦„)
- β… Paged Attention (vLLM)
- β… Multi-Query Attention (GQA)
- β Sparse Attention
- β Linear Attention

## π“‹ Pretraining β†’ vLLM λ°°ν¬ μ²΄ν¬λ¦¬μ¤νΈ

### Phase 1: κ°λ° (qwen_advanced.py)
```python
# 1. λ¨λΈ ν•™μµ
config = AdvancedQwenConfig(...)
model = AdvancedQwenLM(config)

# 2. Pretraining
for epoch in range(num_epochs):
    loss = pretrain_one_epoch(model, dataloader)
    print(f"Loss: {loss}")

# 3. Checkpoint μ €μ¥
torch.save(model.state_dict(), "checkpoint.pt")
```

### Phase 2: λ³€ν™ (qwen_advanced.py β†’ vLLM νΈν™)
```python
# κΈ°μ΅΄ μ²΄ν¬ν¬μΈνΈ λ΅λ“ λ° λ³€ν™
checkpoint = torch.load("checkpoint.pt")
config = AdvancedQwenConfig(...)
vllm_model = AdvancedQwenForCausalLM(config)
vllm_model.load_state_dict(checkpoint)

# HuggingFace ν•μ‹μΌλ΅ μ €μ¥
vllm_model.save_pretrained("./model_hf")
```

### Phase 3: vLLM μ„λΉ™
```bash
# 1. vLLM μ„¤μΉ
pip install vllm

# 2. λ¨λΈ μ„λΉ™
python -m vllm.entrypoints.openai.api_server \
    --model ./model_hf \
    --tensor-parallel-size 4 \
    --max-model-len 4096
```

### Phase 4: API νΈμ¶
```python
import requests

response = requests.post(
    "http://localhost:8000/v1/completions",
    json={
        "model": "default",
        "prompt": "Hello",
        "max_tokens": 100,
    }
)
print(response.json())
```

## β οΈ μ£Όμμ‚¬ν•­

### 1. **State Dict νΈν™μ„±**
```python
# qwen_advanced.pyμ state_dictμ™€
# qwen_vllm_compatible.pyμ state_dictκ°€ λ‹¤λ¥Ό μ μμ
# β†’ λ¨λΈ κµ¬μ΅°λ¥Ό λ™μΌν•κ² μ μ§€ ν•„μ”
```

### 2. **Config νΈν™μ„±**
```python
# Config μ €μ¥ μ‹ λ¨λ“  ν•„λ“κ°€ JSON μ§λ ¬ν™” κ°€λ¥ν•΄μ•Ό ν•¨
# Optional[Dict] νƒ€μ…μ€ NoneμΌ λ• μ²λ¦¬ ν•„μ”
```

### 3. **μ¶”λ΅  vs ν•™μµ λ¨λ“**
```python
# vLLMμ€ μ¶”λ΅ λ§ μ§€μ›
model.eval()  # λ°λ“μ‹ eval λ¨λ“λ΅

# LoRAλ” μ¶”λ΅  μ‹ λΉ„ν™μ„±ν™”
config.use_lora = False  # μ¶”λ΅ μ©
config.use_lora = True   # νμΈνλ‹μ©
```

## π”§ μ¶”κ°€ κ°μ„  μ•„μ΄λ””μ–΄

### 1. **Speculative Decoding** (2λ°° μ†λ„)
- μ‘μ€ λ¨λΈμ΄ λ‹¤μ kκ° ν† ν° μμΈ΅
- ν° λ¨λΈμ΄ κ²€μ¦
- μ¬λ°”λ¥΄λ©΄ kκ° ν† ν° λ™μ‹ μƒμ„±

### 2. **Prefix Caching**
- κ°™μ€ ν”„λ΅¬ν”„νΈλ” μΊμ‹ μ¬μ‚¬μ©
- λ°°μΉ μ¶”λ΅  μ†λ„ 5λ°° μ¦κ°€

### 3. **Mixture of Experts (MoE)**
- λ¶€λ¶„ ν™μ„±ν™” (12.8B β†’ 2B active)
- μ¶”λ΅  λΉ„μ© 90% μ κ°

### 4. **Multi-LoRA**
- μ—¬λ¬ LoRA λ™μ‹ λ΅λ“
- μ‚¬μ©μλ³„ μ»¤μ¤ν„°λ§μ΄μ μ΄μ…

### 5. **Function Calling**
```python
# κµ¬μ΅°ν™”λ μ¶λ ¥
output_format = {
    "type": "object",
    "properties": {
        "function": {"type": "string"},
        "args": {"type": "object"},
    }
}
```

## π“¦ ν”„λ΅λ•μ… λ°°ν¬ κµ¬μ„±

```
qwen-model/
β”β”€β”€ config.json                 # HuggingFace ν•μ‹
β”β”€β”€ generation_config.json      # μƒμ„± μ„¤μ •
β”β”€β”€ pytorch_model.bin           # λ¨λΈ κ°€μ¤‘μΉ (μ—¬λ¬ νμΌ κ°€λ¥)
β”β”€β”€ tokenizer.model            # SentencePiece (μ„ νƒμ‚¬ν•­)
β”β”€β”€ tokenizer.json             # BPE (μ„ νƒμ‚¬ν•­)
β””β”€β”€ special_tokens_map.json    # νΉμ ν† ν° μ •μ
```

## π“ μ„±λ¥ λΉ„κµ

| ν•­λ© | qwen_advanced | vLLM_compatible |
|------|---------------|-----------------|
| λ©”λ¨λ¦¬ | μ‹¤μ  μ‚¬μ© | μµμ ν™”λ¨ |
| μ²λ¦¬λ‰ | κΈ°λ³Έ | 5λ°° μ΄μƒ |
| μ§€μ—°μ‹κ°„ | κΈ°λ³Έ | λ‚®μ |
| μ„λΉ™ μ¤€λΉ„ | X | O |
| νΈν™μ„± | μ ν•μ  | μ™„λ²½ |

## β… μ²΄ν¬λ¦¬μ¤νΈ (λ°°ν¬ μ „)

- [ ] Configκ°€ JSON μ§λ ¬ν™” κ°€λ¥
- [ ] State dictκ°€ vLLMκ³Ό νΈν™
- [ ] Tokenizerκ°€ μ €μ¥λ¨
- [ ] Generation configκ°€ μ„¤μ •λ¨
- [ ] λ¨λΈμ΄ eval λ¨λ“μ—μ„ ν…μ¤νΈλ¨
- [ ] λ©”λ¨λ¦¬ λ„μ μ—†μ (ν”„λ΅νμΌλ§)
- [ ] Batch inference ν…μ¤νΈ μ™„λ£
- [ ] Long sequence ν…μ¤νΈ (μµλ€ κΈΈμ΄)
- [ ] vLLM νΈν™μ„± ν…μ¤νΈ μ™„λ£

## π― κ¶μ¥ μ‚¬ν•­

1. **ν•™μµ λ‹¨κ³„**: `qwen_advanced.py` μ‚¬μ©
2. **λ°°ν¬ λ‹¨κ³„**: `qwen_vllm_compatible.py`λ΅ λ§μ΄κ·Έλ μ΄μ…
3. **μ„λΉ™**: vLLM + OpenAI API νΈν™ μ„λ²„
4. **λ¨λ‹ν„°λ§**: Prometheus + Grafanaλ΅ μ„±λ¥ μ¶”μ 
