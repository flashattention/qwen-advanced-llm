# DeepSeek ìˆ˜ì¤€ LLM êµ¬í˜„ ì™„ì„± ê°€ì´ë“œ

> âš ï¸ **í•™ìŠµìš© í”„ë¡œì íŠ¸**: ì´ í”„ë¡œì íŠ¸ëŠ” GitHub Copilotì„ í™œìš©í•˜ì—¬ í•™ìŠµ ëª©ì ìœ¼ë¡œ ì œì‘ ì¤‘ì…ë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” ì¶”ê°€ ìµœì í™” ë° í…ŒìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.

**ì£¼ìš” ê¸°ëŠ¥**: Flash Attention, GQA, mHC, QLoRA, RoPE Scaling, Continuous Batching ë“± ìµœì‹  LLM ê¸°ìˆ ì´ ëª¨ë‘ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
qwen-advanced-llm/
â”œâ”€â”€ qwen_advanced.py              # í•µì‹¬ ëª¨ë¸ êµ¬í˜„ (í•™ìŠµ/ì‚¬ì „í•™ìŠµìš©)
â”œâ”€â”€ qwen_vllm_compatible.py       # vLLM í˜¸í™˜ ë²„ì „ (ë°°í¬/ì„œë¹™ìš©)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_qlora.py            # QLoRA ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ checkpoints/                  # ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ qwen_model/              # qwen_advanced.py ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ qwen_hf_model/            # qwen_vllm_compatible.py ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ venv/                         # Python 3.13 ê°€ìƒí™˜ê²½
â”œâ”€â”€ requirements.txt              # ì˜ì¡´ì„±: torch>=2.0.0, numpy>=1.20.0
â”œâ”€â”€ README.md                     # ì´ íŒŒì¼
â””â”€â”€ IMPROVEMENTS.md               # ì„±ëŠ¥ ê°œì„  ê³„íš
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (5ë¶„)

### 1ï¸âƒ£ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/flashattention/qwen-advanced-llm.git
cd qwen-advanced-llm

# ê°€ìƒí™˜ê²½ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# ë˜ëŠ”
venv\Scripts\activate     # Windows

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch numpy
```

### 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 

```python
import torch
from qwen_advanced import AdvancedQwenConfig, AdvancedQwenLM, TextGenerator

# ëª¨ë¸ ìƒì„±
config = AdvancedQwenConfig(
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
)
model = AdvancedQwenLM(config)
model.eval()

# ì¶”ë¡ 
with torch.no_grad():
    input_ids = torch.randint(0, 50000, (1, 10))
    logits = model(input_ids)
    print(f"ì¶œë ¥ shape: {logits.shape}")  # (1, 10, 50000)
```

### 3ï¸âƒ£ í…ìŠ¤íŠ¸ ìƒì„±

```python
# TextGenerator ì‚¬ìš© (ìƒ˜í”Œë§ í¬í•¨)
generator = TextGenerator(model, device='cpu')

# Top-p (nucleus) ìƒ˜í”Œë§
generated = generator.generate(
    input_ids=torch.tensor([[1, 2, 3]]),
    max_length=50,
    top_p=0.9,
    temperature=0.7,
)
print(f"ìƒì„±ëœ í† í°: {generated}")
```

## ğŸ“ ìƒì„¸ ì‚¬ìš© ê°€ì´ë“œ

### ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ì½”ë“œ

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ë³¸ ëª¨ë¸ë¡œ í•™ìŠµ

```python
import torch
import torch.nn as nn
from qwen_advanced import AdvancedQwenConfig, AdvancedQwenLM

# ëª¨ë¸ ì„¤ì •
config = AdvancedQwenConfig(
    vocab_size=50000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    max_position_embeddings=2048,
    # ìµœì‹  ê¸°ìˆ  í™œì„±í™”
    use_flash_attention=True,
    use_gqa=True,              # Grouped Query Attention
    use_mhc=True,              # Manifold-Constrained Hyper-Connections
    use_lora=False,            # í•™ìŠµí•  ë•ŒëŠ” False
)

model = AdvancedQwenLM(config)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# ë”ë¯¸ ë°ì´í„°ë¡œ í•™ìŠµ
batch_size, seq_len = 2, 10
input_ids = torch.randint(0, 50000, (batch_size, seq_len))

for epoch in range(3):
    outputs = model(input_ids)
    loss = outputs.mean()  # ì‹¤ì œë¡œëŠ” proper loss function ì‚¬ìš©
    
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), "checkpoints/pretrained_model.pt")
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: QLoRAë¡œ íŒŒì¸íŠœë‹ (ë©”ëª¨ë¦¬ íš¨ìœ¨)

```python
import torch
from qwen_advanced import AdvancedQwenConfig, AdvancedQwenLM

# QLoRA í™œì„±í™” ì„¤ì •
config = AdvancedQwenConfig(
    hidden_size=768,
    num_hidden_layers=12,
    use_qlora=True,            # âœ¨ 4-bit ì–‘ìí™”
    use_lora=True,             # LoRA ì–´ëŒ‘í„°
    lora_rank=8,               # LoRA ë­í¬
    qlora_nf4=True,            # NF4 ì–‘ìí™” (ìµœì‹ )
)

model = AdvancedQwenLM(config)

# ğŸ’¡ í•µì‹¬: LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ!
trainable_params = []
for name, param in model.named_parameters():
    if 'lora' in name.lower():
        param.requires_grad = True
        trainable_params.append(param)
        print(f"í•™ìŠµ ê°€ëŠ¥: {name}")
    else:
        param.requires_grad = False

# í•™ìŠµ ì„¤ì •
optimizer = torch.optim.AdamW(trainable_params, lr=1e-4)
print(f"\nğŸ“Š í•™ìŠµ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in trainable_params):,} ê°œ")
print(f"ğŸ“Š ì „ì²´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,} ê°œ")
print(f"âœ… ë©”ëª¨ë¦¬ ì ˆê°: ~75% (QLoRA ì‚¬ìš©)")

# íŒŒì¸íŠœë‹ ë£¨í”„
for epoch in range(5):
    outputs = model(input_ids)
    loss = outputs.mean()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ

```python
import torch
from qwen_advanced import AdvancedQwenConfig, AdvancedQwenLM

# ì„¤ì • ë‹¤ì‹œ ìƒì„±
config = AdvancedQwenConfig(
    hidden_size=768,
    num_hidden_layers=12,
)

# ëª¨ë¸ ìƒì„±
model = AdvancedQwenLM(config)

# ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
checkpoint = torch.load("checkpoints/pretrained_model.pt", map_location='cpu')
model.load_state_dict(checkpoint, strict=False)

model.eval()
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
```

#### ì‹œë‚˜ë¦¬ì˜¤ 4: HuggingFace í˜•ì‹ìœ¼ë¡œ ì €ì¥/ë¡œë“œ

```python
import torch
from qwen_vllm_compatible import AdvancedQwenConfig, AdvancedQwenForCausalLM

# vLLM í˜¸í™˜ ëª¨ë¸ ìƒì„±
config = AdvancedQwenConfig(
    hidden_size=768,
    num_hidden_layers=12,
)
model = AdvancedQwenForCausalLM(config)

# HuggingFace í˜•ì‹ìœ¼ë¡œ ì €ì¥ (vLLM í˜¸í™˜)
model.save_pretrained("./my_model")
print("âœ… HuggingFace í˜•ì‹ìœ¼ë¡œ ì €ì¥ë¨")

# ë‹¤ì‹œ ë¡œë“œ
loaded_model = AdvancedQwenForCausalLM.from_pretrained("./my_model")
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# í…ìŠ¤íŠ¸ ìƒì„±
input_ids = torch.tensor([[1, 2, 3]])
outputs = loaded_model.generate(
    input_ids, 
    max_length=50,
    top_p=0.9,
)
print(f"ìƒì„±ëœ í† í°: {outputs}")
```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### ëª¨ë“  ì„¤ì • ì˜µì…˜

```python
from qwen_advanced import AdvancedQwenConfig

config = AdvancedQwenConfig(
    # === ê¸°ë³¸ ì„¤ì • ===
    vocab_size=50000,              # ì–´íœ˜ í¬ê¸°
    hidden_size=768,               # íˆë“  ì°¨ì›
    num_hidden_layers=12,          # ë ˆì´ì–´ ìˆ˜
    num_attention_heads=12,        # ì–´í…ì…˜ í—¤ë“œ ìˆ˜
    intermediate_size=3072,        # FFN ì¤‘ê°„ í¬ê¸°
    max_position_embeddings=2048,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    
    # === ìµœì í™” ê¸°ìˆ  ===
    use_flash_attention=True,      # Flash Attention (3ë°° ë¹ ë¦„)
    use_gqa=True,                  # Grouped Query Attention (KV ìºì‹œ 75% ì ˆê°)
    num_kv_heads=4,                # GQA ì‹œ KV í—¤ë“œ ìˆ˜
    use_mhc=True,                  # mHC (DeepSeek ê¸°ìˆ )
    mhc_num_streams=4,             # mHC ìŠ¤íŠ¸ë¦¼ ìˆ˜
    
    # === QLoRA (ë©”ëª¨ë¦¬ íš¨ìœ¨) ===
    use_lora=True,                 # LoRA ì–´ëŒ‘í„°
    use_qlora=True,                # 4-bit ì–‘ìí™”
    lora_rank=8,                   # LoRA ë­í¬ (ì‘ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ)
    lora_alpha=16.0,               # LoRA ìŠ¤ì¼€ì¼
    qlora_nf4=True,                # NF4 ì–‘ìí™” (ìµœì‹ )
    
    # === RoPE Scaling ===
    rope_scaling={                 # ê¸´ ì‹œí€€ìŠ¤ ì§€ì›
        "type": "linear",
        "factor": 1.0,
    },
)
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| ê¸°ìˆ  | íš¨ê³¼ | ë©”ëª¨ë¦¬ | ì†ë„ |
|------|------|--------|------|
| ê¸°ë³¸ Attention | - | 1x | 1x |
| + Flash Attention | IO ìµœì í™” | 1x | **3x** |
| + GQA | KV ê³µìœ  | **0.75x** | 3x |
| + QLoRA | 4-bit ì–‘ìí™” | **0.3x** | 3x |
| **ì „ì²´ ìµœì í™”** | ëª¨ë‘ ì ìš© | **0.25x** | **10x** |

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# QLoRA ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
cd qwen-advanced-llm
source venv/bin/activate
python tests/test_qlora.py

# ì¶œë ¥:
# === QLoRA í…ŒìŠ¤íŠ¸ ===
# âœ… QLoRA ìˆœì „íŒŒ: torch.Size([2, 10, 768])
# âœ… QLoRALinear ìˆœì „íŒŒ: torch.Size([2, 10, 768])
# ...
# ğŸ‰ QLoRA êµ¬í˜„ ì™„ë£Œ!
```

## ğŸš¨ ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

### Q1: ModuleNotFoundError: torch

```bash
# í•´ê²°: PyTorch ì„¤ì¹˜
pip install torch

# ë˜ëŠ” CUDA ì§€ì› ë²„ì „
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Q2: CUDA Out of Memory

```python
# ë°©ë²• 1: Batch size ì¤„ì´ê¸°
batch_size = 1  # 2 ëŒ€ì‹  1

# ë°©ë²• 2: QLoRA í™œì„±í™” (ë©”ëª¨ë¦¬ 75% ì ˆê°)
config = AdvancedQwenConfig(use_qlora=True, use_lora=True)

# ë°©ë²• 3: Gradient checkpointing (PyTorch)
torch.utils.checkpoint.checkpoint(layer, hidden_states)
```

### Q3: ëª¨ë¸ì´ ëŠë¦¼

```python
# ë°©ë²• 1: Flash Attention í™•ì¸
assert config.use_flash_attention == True

# ë°©ë²• 2: GQA í™œì„±í™” (ë©”ëª¨ë¦¬+ì†ë„)
config = AdvancedQwenConfig(use_gqa=True)

# ë°©ë²• 3: fp32 ëŒ€ì‹  fp16/bf16 ì‚¬ìš©
model = model.half()  # fp16
```

### Q4: ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜

```python
# ë°©ë²•: strict=False ì‚¬ìš© (í˜¸í™˜ì„±)
checkpoint = torch.load("model.pt")
model.load_state_dict(checkpoint, strict=False)
# strict=Falseë©´ ì¼ë¶€ ë ˆì´ì–´ ë¶ˆì¼ì¹˜ ë¬´ì‹œ
```

## ğŸ” ê¸°ìˆ  ìƒì„¸ ì„¤ëª…

### Flash Attentionì´ë€?
- **ë¬¸ì œ**: ê¸°ë³¸ Attentionì€ ë©”ëª¨ë¦¬ ì ‘ê·¼ì´ ë¹„íš¨ìœ¨ì 
- **í•´ê²°**: Block-wise ê³„ì‚°ìœ¼ë¡œ IO ìµœì í™”
- **íš¨ê³¼**: ê°™ì€ ë©”ëª¨ë¦¬ì—ì„œ 3ë°° ë¹ ë¦„

### GQA (Grouped Query Attention)ì´ë€?
- **ë¬¸ì œ**: KV ìºì‹œê°€ ë„ˆë¬´ í¼ (ì „ì²´ ë©”ëª¨ë¦¬ì˜ 40%)
- **í•´ê²°**: ì—¬ëŸ¬ Queryê°€ í•˜ë‚˜ì˜ KV í—¤ë“œ ê³µìœ 
- **íš¨ê³¼**: ë©”ëª¨ë¦¬ 75% ì ˆê°, ì •í™•ë„ ìœ ì§€

### QLoRAë€?
- **ë¬¸ì œ**: LoRAë„ ë©”ëª¨ë¦¬ ë§ì´ ì”€
- **í•´ê²°**: ê°€ì¤‘ì¹˜ë¥¼ 4-bitìœ¼ë¡œ ì–‘ìí™” + LoRA ì–´ëŒ‘í„°
- **íš¨ê³¼**: ë©”ëª¨ë¦¬ 4ë°° ì ˆê°, í•™ìŠµ ê°€ëŠ¥

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„



1. **ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ**
   - í† í¬ë‚˜ì´ì € ì¤€ë¹„
   - ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì„±
   - Batch ì²˜ë¦¬ ìµœì í™”

2. **ëª¨ë¸ í‰ê°€**
   - Perplexity ì¸¡ì •
   - Benchmark ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
   - ì¶”ë¡  ì†ë„ í”„ë¡œíŒŒì¼ë§

3. **ë°°í¬**
   - vLLM ì„œë¹™
   - API ê²Œì´íŠ¸ì›¨ì´ ì„¤ì •
   - ëª¨ë‹ˆí„°ë§ êµ¬ì¶•

## ğŸ“– í•µì‹¬ ê¸°ìˆ  ì°¸ê³ ë¬¸í—Œ

| ê¸°ìˆ  | ë…¼ë¬¸ | íš¨ê³¼ |
|------|------|------|
| **Flash Attention** | [arxiv:2205.14135](https://arxiv.org/abs/2205.14135) | ì¶”ë¡  3ë°° ë¹ ë¦„ |
| **GQA** | [arxiv:2305.13245](https://arxiv.org/abs/2305.13245) | ë©”ëª¨ë¦¬ 75% ì ˆê° |
| **mHC** | [arxiv:2601.05732](https://arxiv.org/abs/2601.05732) | í•™ìŠµ ì•ˆì •ì„± |
| **LoRA** | [arxiv:2106.09685](https://arxiv.org/abs/2106.09685) | íŒŒì¸íŠœë‹ íš¨ìœ¨ |
| **RoPE** | [arxiv:2104.09864](https://arxiv.org/abs/2104.09864) | ê¸´ ì‹œí€€ìŠ¤ ì§€ì› |

## ğŸ’» ê°œë°œ íŒ

### IDE ì„¤ì • (VS Code)

`.vscode/settings.json`:
```json
{
  "python.defaultInterpreterPath": "${workspaceFolder}/venv/bin/python",
  "python.linting.enabled": true,
  "python.linting.pylintEnabled": true,
  "python.formatting.provider": "black",
  "[python]": {
    "editor.defaultFormatter": "ms-python.python",
    "editor.formatOnSave": true
  }
}
```

### ë””ë²„ê¹…

```python
# ëª¨ë¸ êµ¬ì¡° í™•ì¸
print(model)

# íŒŒë¼ë¯¸í„° í™•ì¸
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total: {total_params:,}, Trainable: {trainable_params:,}")

# Gradient í™•ì¸
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm():.4f}")
```

### ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§

```python
import time
import torch

# ì¶”ë¡  ì‹œê°„ ì¸¡ì •
model.eval()
with torch.no_grad():
    start = time.time()
    for _ in range(10):
        outputs = model(input_ids)
    elapsed = time.time() - start
    print(f"ì¶”ë¡  ì‹œê°„: {elapsed/10:.4f}ì´ˆ")

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
print(f"ë©”ëª¨ë¦¬: {torch.cuda.max_memory_allocated() / 1e9:.2f}GB")
```

## âœ¨ ì£¼ìš” íŠ¹ì§• ìš”ì•½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AdvancedQwenLM (í•™ìŠµ/ê°œë°œìš©)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Flash Attention (3ë°° ë¹ ë¦„)       â”‚
â”‚ âœ… GQA (ë©”ëª¨ë¦¬ 75% ì ˆê°)           â”‚
â”‚ âœ… mHC (í•™ìŠµ ì•ˆì •ì„±)               â”‚
â”‚ âœ… QLoRA (4-bit ì–‘ìí™”)            â”‚
â”‚ âœ… RoPE Scaling (8K í† í°)          â”‚
â”‚ âœ… Continuous Batching             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ ë³€í™˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AdvancedQwenForCausalLM (ë°°í¬ìš©)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… HuggingFace í˜¸í™˜                 â”‚
â”‚ âœ… vLLM ìµœì í™”                      â”‚
â”‚ âœ… Generation API                   â”‚
â”‚ âœ… Top-p/Top-k ìƒ˜í”Œë§               â”‚
â”‚ âœ… ëª¨ë¸ ì €ì¥/ë¡œë“œ                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤ ê¸°ì—¬ ê°€ì´ë“œ

```bash
# ì´ ì €ì¥ì†Œë¥¼ í¬í¬ í›„
git clone https://github.com/YOUR_USERNAME/qwen-advanced-llm.git
git checkout -b feature/ìƒˆê¸°ëŠ¥
# ì½”ë“œ ì‘ì„±
git add .
git commit -m "feat: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€"
git push origin feature/ìƒˆê¸°ëŠ¥
# Pull Request ìƒì„±
```

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” í•™ìŠµ ëª©ì ìœ¼ë¡œ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ™ ê°ì‚¬ì˜ ë§

- Meta (Flash Attention)
- Google (GQA)
- DeepSeek (mHC)
- Microsoft (LoRA)
- ê·¸ë¦¬ê³  GitHub Copilot

## ğŸ“§ ì§ˆë¬¸ ë° í”¼ë“œë°±

- Issues: GitHub Issues íƒ­ì—ì„œ ë²„ê·¸ ë³´ê³ 
- Discussions: ì•„ì´ë””ì–´ ê³µìœ  ë° ì§ˆë¬¸
- Email: ì§ì ‘ ì—°ë½ í•„ìš” ì‹œ

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2026ë…„ 1ì›” 19ì¼  
**ë²„ì „**: 1.0.0 (QLoRA êµ¬í˜„ ì™„ë£Œ)
