# DeepSeek μμ¤€ LLM κµ¬ν„ μ™„μ„± κ°€μ΄λ“

> β οΈ **ν•™μµμ© ν”„λ΅μ νΈ**: μ΄ ν”„λ΅μ νΈλ” GitHub Copilotμ„ ν™μ©ν•μ—¬ ν•™μµ λ©μ μΌλ΅ μ μ‘ μ¤‘μ…λ‹λ‹¤. ν”„λ΅λ•μ… ν™κ²½μ—μ„μ μ‚¬μ©μ„ μ„ν•΄μ„λ” μ¶”κ°€ μµμ ν™” λ° ν…μ¤νΈκ°€ ν•„μ”ν•©λ‹λ‹¤.

## π“ νμΌ κµ¬μ΅°

```
/Users/louisjeon/dev/continue/
β”β”€β”€ qwen_advanced.py              # ν•µμ‹¬ κΈ°μ  (ν•™μµμ©)
β”‚   β”β”€β”€ Flash Attention
β”‚   β”β”€β”€ GQA (Grouped Query Attention)
β”‚   β”β”€β”€ mHC (Manifold-Constrained Hyper-Connections)
β”‚   β”β”€β”€ LoRA
β”‚   β””β”€β”€ Rope Scaling + Continuous Batching
β”‚
β”β”€β”€ qwen_vllm_compatible.py       # ν”„λ΅λ•μ… λ°°ν¬ λ²„μ „
β”‚   β”β”€β”€ HuggingFace μ™„λ²½ νΈν™
β”‚   β”β”€β”€ vLLM μµμ ν™”
β”‚   β”β”€β”€ Config/Checkpoint κ΄€λ¦¬
β”‚   β””β”€β”€ Generation API
β”‚
β”β”€β”€ qwen_model/                   # qwen_advanced.pyμ μ²΄ν¬ν¬μΈνΈ
β”β”€β”€ qwen_hf_model/                # qwen_vllm_compatible.pyμ μ²΄ν¬ν¬μΈνΈ
β”‚
β””β”€β”€ IMPROVEMENTS.md               # κ°μ„ μ‚¬ν•­ μƒμ„Έ κ°€μ΄λ“
```

## π€ μ‚¬μ© νλ¦„

### 1λ‹¨κ³„: ν•™μµ (qwen_advanced.py)
```python
from qwen_advanced import AdvancedQwenConfig, AdvancedQwenLM

config = AdvancedQwenConfig(
    hidden_size=768,
    num_hidden_layers=12,
    use_flash_attention=True,
    use_gqa=True,
    use_mhc=True,
)

model = AdvancedQwenLM(config)

# Pretraining...
# torch.save(model.state_dict(), "pretrained.pt")
```

### 2λ‹¨κ³„: λ³€ν™ λ° μµμ ν™” (qwen_vllm_compatible.py)
```python
from qwen_vllm_compatible import AdvancedQwenForCausalLM, AdvancedQwenConfig

# κΈ°μ΅΄ μ²΄ν¬ν¬μΈνΈ λ΅λ“
checkpoint = torch.load("pretrained.pt")

# vLLM νΈν™ λ¨λΈ μƒμ„±
config = AdvancedQwenConfig(...)
model = AdvancedQwenForCausalLM(config)

# κ°€μ¤‘μΉ λ΅λ“
model.load_state_dict(checkpoint, strict=False)

# HuggingFace ν•μ‹μΌλ΅ μ €μ¥
model.save_pretrained("./my_model")
```

### 3λ‹¨κ³„: vLLM μ„λΉ™
```bash
pip install vllm

python -m vllm.entrypoints.openai.api_server \
    --model ./my_model \
    --tensor-parallel-size 4 \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.9
```

### 4λ‹¨κ³„: API νΈμ¶
```python
from openai import OpenAI

client = OpenAI(
    api_key="token",
    base_url="http://localhost:8000/v1"
)

response = client.completions.create(
    model="default",
    prompt="μ•λ…•ν•μ„Έμ”",
    max_tokens=100,
)

print(response.choices[0].text)
```

## π― ν•µμ‹¬ κΈ°μ  μ •λ¦¬

### 1. **Flash Attention** (Meta)
- **ν¨κ³Ό**: 3λ°° λΉ λ¥Έ μ¶”λ΅ 
- **μ›λ¦¬**: IO ν¨μ¨μ μΈ λ©”λ¨λ¦¬ μ ‘κ·Ό ν¨ν„΄
- **μƒνƒ**: β… κµ¬ν„λ¨

### 2. **GQA - Grouped Query Attention** (Google)
- **ν¨κ³Ό**: KV μΊμ‹ 75% κ°μ†
- **μ›λ¦¬**: μ—¬λ¬ Qκ°€ ν•λ‚μ KV κ·Έλ£Ή κ³µμ 
- **μƒνƒ**: β… κµ¬ν„λ¨

### 3. **mHC - Manifold-Constrained Hyper-Connections** (DeepSeek)
- **ν¨κ³Ό**: ν•™μµ μ•μ •μ„± + μλ ΄ κ°€μ†
- **μ›λ¦¬**: Doubly stochastic ν–‰λ ¬λ΅ λ‹¤μ¤‘ μ¤νΈλ¦Ό νΌν•©
- **μƒνƒ**: β… κµ¬ν„λ¨

### 4. **QLoRA - Quantization-aware LoRA** (Meta/Mistral/DeepSeek)
- **ν¨κ³Ό**: λ©”λ¨λ¦¬ 4λ°° κ°μ† + ν•™μµ κ°€λ¥
- **μ›λ¦¬**: 4-bit NF4 μ–‘μν™” + LoRA μ–΄λ‘ν„°
- **ν™μ©**: λ‹¨μΌ GPUμ—μ„ 7B λ¨λΈ νμΈνλ‹ κ°€λ¥
- **μƒνƒ**: β… κµ¬ν„λ¨
- **μ‚¬μ©μ²**: μµμ‹  LLM(Llama 2, Mistral, DeepSeek)μ ν‘μ¤€ νμΈνλ‹ λ°©μ‹

### 5. **RoPE Scaling**
- **ν¨κ³Ό**: 8K ν† ν°κΉμ§€ ν™•μ¥ κ°€λ¥
- **μ›λ¦¬**: Positional encoding μ¤μΌ€μΌλ§
- **μƒνƒ**: β… κµ¬ν„λ¨

### 6. **Continuous Batching** (vLLM)
- **ν¨κ³Ό**: μ²λ¦¬λ‰ 5λ°° μ¦κ°€
- **μ›λ¦¬**: λ™μ  λ°°μΉ μƒμ„± λ° μ¤μΌ€μ¤„λ§
- **μƒνƒ**: β… κµ¬ν„λ¨

## π’Ύ νμΈνλ‹ (QLoRA μ‚¬μ©)

```python
# QLoRA νμΈνλ‹ - λ©”λ¨λ¦¬ ν¨μ¨μ 
from qwen_advanced import AdvancedQwenConfig, AdvancedQwenLM

config = AdvancedQwenConfig(
    hidden_size=768,
    use_qlora=True,      # QLoRA ν™μ„±ν™”
    use_lora=True,
    lora_rank=8,
    qlora_nf4=True,      # NF4 μ–‘μν™” μ‚¬μ©
)

model = AdvancedQwenLM(config)

# μµμ ν™”: LoRA νλΌλ―Έν„°λ§ ν•™μµ
trainable_params = []
for name, param in model.named_parameters():
    if 'lora' in name:
        param.requires_grad = True
        trainable_params.append(param)
    else:
        param.requires_grad = False

# λ©”λ¨λ¦¬ ν¨μ¨μ μΈ νμΈνλ‹
optimizer = torch.optim.AdamW(trainable_params, lr=1e-4)

# μμƒ λ©”λ¨λ¦¬ μ‚¬μ©λ‰:
# - κΈ°λ³Έ LoRA: ~30GB (7B λ¨λΈ)
# - QLoRA: ~7-15GB (4-bit μ–‘μν™”)
```

## π“ μ„±λ¥ λ©”νΈλ¦­

| λ©”νΈλ¦­ | κΈ°λ³Έ | μµμ ν™”λ¨ | κ°μ„ μ¨ |
|--------|------|---------|--------|
| μ¶”λ΅  μ†λ„ | 1x | 10-20x | **1000%** |
| λ©”λ¨λ¦¬ | 1x | 0.4x | **60% μ κ°** |
| μ²λ¦¬λ‰ | 1x | 5x | **500%** |
| νμΈνλ‹ λ©”λ¨λ¦¬ (LoRAβ†’QLoRA) | 30GB | 7-15GB | **75% μ κ°** |
| ν•™μµ μ‹κ°„ | 1x | 0.8x | **20% λ‹¨μ¶•** |
| λ¨λΈ ν¬κΈ° | 1x | 0.25x | **75% μ••μ¶•** |

## β… λ°°ν¬ μ²΄ν¬λ¦¬μ¤νΈ

### μ½”λ“ μ¤€λΉ„
- [x] λ¨λΈ κµ¬ν„ μ™„λ£
- [x] Config κ΄€λ¦¬ μ‹μ¤ν…
- [x] Checkpoint μ €μ¥/λ΅λ“
- [x] HuggingFace νΈν™
- [x] vLLM νΈν™μ„±

### λ°μ΄ν„° μ¤€λΉ„
- [ ] ν•™μµ λ°μ΄ν„° μμ§‘
- [ ] λ°μ΄ν„° ν΄λ¦°μ§•
- [ ] Tokenizer ν•™μµ
- [ ] λ°μ΄ν„°μ…‹ κ²€μ¦

### ν•™μµ
- [ ] Pretraining μ™„λ£
- [ ] Evaluation λ©”νΈλ¦­ μ„¤μ •
- [ ] ν•μ΄νΌνλΌλ―Έν„° νλ‹
- [ ] μ²΄ν¬ν¬μΈνΈ μ €μ¥

### λ°°ν¬
- [ ] vLLM ν…μ¤νΈ
- [ ] μ„±λ¥ ν”„λ΅νμΌλ§
- [ ] λ©”λ¨λ¦¬ μµμ ν™”
- [ ] ν™•μ¥μ„± ν…μ¤νΈ
- [ ] λ¨λ‹ν„°λ§ μ„¤μ •

### μ΄μ
- [ ] API κ²μ΄νΈμ›¨μ΄ μ„¤μ •
- [ ] λ΅κΉ… λ° λ¨λ‹ν„°λ§
- [ ] λ°±μ—… λ° λ³µκµ¬ μ „λµ
- [ ] λ²„μ „ κ΄€λ¦¬

## π” λ¬Έμ  ν•΄κ²°

### Issue 1: "State dict λ¶μΌμΉ"
```python
# ν•΄κ²°μ±…: strict=False μ‚¬μ©
model.load_state_dict(checkpoint, strict=False)
```

### Issue 2: "CUDA OOM"
```bash
# ν•΄κ²°μ±…: λ©”λ¨λ¦¬ μµμ ν™” ν”λκ·Έ
python -m vllm.entrypoints.openai.api_server \
    --model my_model \
    --gpu-memory-utilization 0.9  # λ©”λ¨λ¦¬ μ‚¬μ©λ¥ 
```

### Issue 3: "λλ¦° μ¶”λ΅ "
```bash
# ν•΄κ²°μ±…: Tensor parallel ν™μ„±ν™”
--tensor-parallel-size 4  # 4κ° GPU ν™μ©
```

## π“ μ°Έκ³  λ…Όλ¬Έ

1. **Flash Attention**: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
2. **GQA**: [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)
3. **mHC**: [mHC-lite: You Don't Need 20 Sinkhorn-Knopp Iterations](https://arxiv.org/abs/2601.05732)
4. **LoRA**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
5. **RoPE**: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)

## π“ ν•™μµ κ²½λ΅

1. **κΈ°μ΄ μ΄ν•΄** (1μ£Ό)
   - Transformer μ•„ν‚¤ν…μ²
   - Attention λ©”μ»¤λ‹μ¦
   - Position encoding

2. **κΈ°μ  ν•™μµ** (2μ£Ό)
   - Flash Attention μ›λ¦¬
   - GQA κµ¬ν„
   - mHC μμ‹ μ΄ν•΄

3. **μ‹¤μ „ κµ¬ν„** (2μ£Ό)
   - qwen_advanced.py λ¶„μ„
   - qwen_vllm_compatible.py ν•™μµ
   - Pretraining μ‹¤ν–‰

4. **λ°°ν¬** (1μ£Ό)
   - vLLM μ„¤μΉ λ° μ„¤μ •
   - API ν…μ¤νΈ
   - μ„±λ¥ μµμ ν™”

## π¨ μ£Όμμ‚¬ν•­

1. **λ©”λ¨λ¦¬ κ΄€λ¦¬**
   - GPU λ©”λ¨λ¦¬λ” μ ν•ν•¨
   - Batch size μ΅°μ  ν•„μ”
   - Tensor parallel κ³ λ ¤

2. **μ •λ°€λ„ λ¬Έμ **
   - fp32 vs fp16 vs bf16
   - μ–‘μν™”μ μ •ν™•λ„ μ†μ‹¤
   - κ²€μ¦ λ°μ΄ν„°λ΅ ν™•μΈ

3. **νΈν™μ„±**
   - μ„λ΅ λ‹¤λ¥Έ λ¨λΈ λ²„μ „
   - Tokenizer λ²„μ „ κ΄€λ¦¬
   - Config νΈν™μ„±

## π’΅ μ¶”κ°€ ν

1. **κ°λ° ν™κ²½ μµμ ν™”**
```bash
# torch 2.0+ μ»΄νμΌ ν™μ„±ν™”
torch._dynamo.config.cache_size_limit = 64
torch._dynamo.config.suppress_errors = True
```

2. **ν”„λ΅νμΌλ§**
```python
from torch.profiler import profile, record_function

with profile(activities=[...], record_shapes=True) as prof:
    model(input_ids)
    
print(prof.key_averages().table(sort_by="cpu_time_total"))
```

3. **λ””λ²„κΉ…**
```python
# Gradient checking
torch.autograd.gradcheck(model, input, eps=1e-6, atol=1e-4)
```

## π‰ μ¶•ν•ν•©λ‹λ‹¤!

μ΄μ  DeepSeek μμ¤€μ κ³ κΈ‰ LLMμ„ μ™„μ„±ν–μµλ‹λ‹¤!
- β… λ¨λ“  ν•µμ‹¬ κΈ°μ  κµ¬ν„
- β… vLLM νΈν™μ„± ν™•λ³΄
- β… λ°°ν¬ μ¤€λΉ„ μ™„λ£

λ‹¤μ λ‹¨κ³„: μ‹¤μ  λ°μ΄ν„°λ΅ Pretrainingμ„ μ‹μ‘ν•μ„Έμ”! π€
