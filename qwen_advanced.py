"""
ê³ ê¸‰ ê¸°ëŠ¥ì„ í¬í•¨í•œ ì‹¤ë¬´ê¸‰ LLM
- Flash Attention (ë¹ ë¥¸ ì¶”ë¡ )
- Paged Attention (ë©”ëª¨ë¦¬ íš¨ìœ¨)
- LoRA (íŒŒì¸íŠœë‹)
- ì–‘ìí™” (ëª¨ë¸ ì••ì¶•)
- Continuous Batching (ë†’ì€ ì²˜ë¦¬ëŸ‰)
- Rope Scaling (ê¸´ ì»¨í…ìŠ¤íŠ¸)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List, Dict, Any
from dataclasses import dataclass
import math
from collections import deque


@dataclass
class AdvancedQwenConfig:
    """ê³ ê¸‰ ê¸°ëŠ¥ì„ í¬í•¨í•œ ì„¤ì •"""
    vocab_size: int = 50000
    hidden_size: int = 768
    num_hidden_layers: int = 12
    num_attention_heads: int = 12
    intermediate_size: int = 3072
    max_position_embeddings: int = 2048
    
    # Flash Attention
    use_flash_attention: bool = True
    
    # Rope Scaling
    use_gqa: bool = True
    num_kv_heads: int = 4  # num_attention_headsë¥¼ num_kv_headsë¡œ ì¤„ì„
    
    # mHC (Manifold-Constrained Hyper-Connections)
    use_mhc: bool = True
    mhc_num_streams: int = 4
    
    # LoRA
    use_lora: bool = True
    lora_rank: int = 8
    lora_alpha: float = 16.0
    
    # Rope Scaling
    rope_scaling: Optional[Dict[str, Any]] = None  # {"type": "linear", "factor": 2.0}
    
    # ì–‘ìí™”
    quantization: Optional[str] = None  # "int8" or "int4"


class GroupedQueryAttention(nn.Module):
    """GQA (Grouped Query Attention) - ë©”ëª¨ë¦¬ì™€ ì†ë„ ìµœì í™”"""
    
    def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: Optional[int] = None, dropout: float = 0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads or num_heads
        self.head_dim = hidden_size // num_heads
        
        # ì¿¼ë¦¬ëŠ” num_headsë§Œí¼, K/VëŠ” num_kv_headsë§Œí¼
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        self.k_proj = nn.Linear(hidden_size, (hidden_size // num_heads) * self.num_kv_heads)
        self.v_proj = nn.Linear(hidden_size, (hidden_size // num_heads) * self.num_kv_heads)
        self.o_proj = nn.Linear(hidden_size, hidden_size)
        
        self.dropout_p = dropout
        self.num_groups = num_heads // self.num_kv_heads
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        GQA: ì—¬ëŸ¬ ì¿¼ë¦¬ê°€ í•˜ë‚˜ì˜ K/V ê·¸ë£¹ì„ ê³µìœ 
        ë©”ëª¨ë¦¬: O(n*d) â†’ O(n*d/g) (g = ê·¸ë£¹ ìˆ˜)
        ì†ë„: KV ìºì‹œ í¬ê¸° 1/gìœ¼ë¡œ ê°ì†Œ
        """
        batch_size, seq_len, _ = hidden_states.shape
        
        q = self.q_proj(hidden_states)
        k = self.k_proj(hidden_states)
        v = self.v_proj(hidden_states)
        
        # Q: (batch, seq_len, num_heads, head_dim)
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # K, V: (batch, seq_len, num_kv_heads, head_dim)
        k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
        
        # K, Vë¥¼ num_heads í¬ê¸°ë¡œ í™•ì¥ (ê·¸ë£¹ ë°˜ë³µ)
        k = k.repeat_interleave(self.num_groups, dim=1)
        v = v.repeat_interleave(self.num_groups, dim=1)
        
        # Attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if attention_mask is not None:
            scores = scores + attention_mask.unsqueeze(1).unsqueeze(1)
        
        attn_weights = F.softmax(scores, dim=-1)
        
        if self.training:
            attn_weights = F.dropout(attn_weights, p=self.dropout_p, training=True)
        
        output = torch.matmul(attn_weights, v)
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, seq_len, self.hidden_size)
        output = self.o_proj(output)
        
        return output


class FlashAttention(nn.Module):
    """Flash Attention - IO íš¨ìœ¨ì ì¸ ì–´í…ì…˜"""
    
    def __init__(self, hidden_size: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        self.k_proj = nn.Linear(hidden_size, hidden_size)
        self.v_proj = nn.Linear(hidden_size, hidden_size)
        self.o_proj = nn.Linear(hidden_size, hidden_size)
        
        self.dropout_p = dropout
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Flash Attention êµ¬í˜„
        ìµœì í™”ëœ ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ìœ¼ë¡œ ë¹ ë¥¸ ì—°ì‚°
        """
        batch_size, seq_len, _ = hidden_states.shape
        
        q = self.q_proj(hidden_states)
        k = self.k_proj(hidden_states)
        v = self.v_proj(hidden_states)
        
        # Multi-head reshape
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Flash Attention: ë¸”ë¡ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        block_size = min(128, seq_len)  # ë¸”ë¡ í¬ê¸°
        output = torch.zeros_like(q)
        
        for start in range(0, seq_len, block_size):
            end = min(start + block_size, seq_len)
            q_block = q[:, :, start:end, :]
            
            # Attention ê³„ì‚°
            scores = torch.matmul(q_block, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
            
            if attention_mask is not None:
                scores = scores + attention_mask[:, :, start:end, :].unsqueeze(1)
            
            attn_weights = F.softmax(scores, dim=-1)
            
            if self.training:
                attn_weights = F.dropout(attn_weights, p=self.dropout_p, training=True)
            
            block_output = torch.matmul(attn_weights, v)
            output[:, :, start:end, :] = block_output
        
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, seq_len, self.hidden_size)
        output = self.o_proj(output)
        
        return output


class PagedAttention(nn.Module):
    """Paged Attention - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ KV ìºì‹œ"""
    
    def __init__(self, hidden_size: int, num_heads: int, page_size: int = 16):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.page_size = page_size
        
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        self.k_proj = nn.Linear(hidden_size, hidden_size)
        self.v_proj = nn.Linear(hidden_size, hidden_size)
        self.o_proj = nn.Linear(hidden_size, hidden_size)
        
        # KV ìºì‹œ (í˜ì´ì§€ ê¸°ë°˜)
        self.kv_cache = {}
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        cache_key: Optional[str] = None,
    ) -> Tuple[torch.Tensor, Optional[Dict]]:
        """
        Paged Attention: KV ìºì‹œë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬
        ë©”ëª¨ë¦¬ í• ë‹¹ íš¨ìœ¨ì„± â†‘
        """
        batch_size, seq_len, _ = hidden_states.shape
        
        q = self.q_proj(hidden_states)
        k = self.k_proj(hidden_states)
        v = self.v_proj(hidden_states)
        
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Paged KV ìºì‹œ ì—…ë°ì´íŠ¸
        if cache_key and cache_key in self.kv_cache:
            cached_k, cached_v = self.kv_cache[cache_key]
            k = torch.cat([cached_k, k], dim=-2)
            v = torch.cat([cached_v, v], dim=-2)
        
        # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ìºì‹œ ì €ì¥ (ë§¤ page_size í† í°ë§ˆë‹¤)
        if cache_key and seq_len % self.page_size == 0:
            self.kv_cache[cache_key] = (k.detach(), v.detach())
        
        # Attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if attention_mask is not None:
            scores = scores + attention_mask.unsqueeze(1).unsqueeze(1)
        
        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, v)
        
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, seq_len, self.hidden_size)
        output = self.o_proj(output)
        
        cache_output = {cache_key: (k, v)} if cache_key else None
        return output, cache_output


class LoRA(nn.Module):
    """Low-Rank Adaptation - íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹"""
    
    def __init__(self, in_features: int, out_features: int, rank: int = 8, alpha: float = 16.0):
        super().__init__()
        self.lora_a = nn.Linear(in_features, rank, bias=False)
        self.lora_b = nn.Linear(rank, out_features, bias=False)
        
        self.scaling = alpha / rank
        
        # ì´ˆê¸°í™”
        nn.init.kaiming_uniform_(self.lora_a.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_b.weight)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """LoRA ê³„ì‚°: Y = X + (X @ A @ B) * scaling"""
        return (self.lora_b(self.lora_a(x))) * self.scaling


class LoRALinear(nn.Module):
    """LoRAê°€ ì ìš©ëœ ì„ í˜• ë ˆì´ì–´"""
    
    def __init__(self, in_features: int, out_features: int, rank: int = 8, use_lora: bool = True):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.lora = LoRA(in_features, out_features, rank) if use_lora else None
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self.linear(x)
        if self.lora is not None:
            out = out + self.lora(x)
        return out


class RopeScaling(nn.Module):
    """í™•ì¥ëœ RoPE - ë” ê¸´ ì‹œí€€ìŠ¤ ì§€ì›"""
    
    def __init__(
        self,
        dim: int,
        max_position_embeddings: int = 2048,
        base: float = 10000.0,
        scaling_config: Optional[Dict[str, Any]] = None,
    ):
        super().__init__()
        self.dim = dim
        self.base = base
        self.max_position_embeddings = max_position_embeddings
        
        # Scaling ì„¤ì • (ê¸´ ì‹œí€€ìŠ¤ ì§€ì›)
        self.scaling_config = scaling_config or {"type": "linear", "factor": 1.0}
        
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
    
    def forward(self, q: torch.Tensor, k: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:
        t = torch.arange(seq_len, device=q.device, dtype=torch.float32)
        
        # Rope Scaling ì ìš©
        if self.scaling_config["type"] == "linear":
            factor = self.scaling_config.get("factor", 1.0)
            t = t / factor
        
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        emb = torch.cat([freqs, freqs], dim=-1)
        cos = emb.cos()[None, None, :, :]
        sin = emb.sin()[None, None, :, :]
        
        q_rot = (q * cos) + (rotate_half(q) * sin)
        k_rot = (k * cos) + (rotate_half(k) * sin)
        return q_rot, k_rot


def rotate_half(x: torch.Tensor) -> torch.Tensor:
    x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
    return torch.cat([-x2, x1], dim=-1)


class ManifoldConstrainedHyperConnections(nn.Module):
    """
    mHC (Manifold-Constrained Hyper-Connections)
    
    ì°¸ê³ : DeepSeekì˜ mHC ë…¼ë¬¸
    - ë™ì  residual í–‰ë ¬ë¡œ ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼ ì •ë³´ í˜¼í•©
    - Birkhoff polytope ì œì•½ (doubly stochastic í–‰ë ¬)
    - í•™ìŠµ ì•ˆì •ì„± ë° ìˆ˜ë ´ ì†ë„ í–¥ìƒ
    """
    
    def __init__(self, hidden_size: int, num_streams: int = 4):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_streams = num_streams
        
        # ê° ìŠ¤íŠ¸ë¦¼ì— ëŒ€í•œ ì„ í˜• ë³€í™˜
        self.stream_projections = nn.ModuleList([
            nn.Linear(hidden_size, hidden_size)
            for _ in range(num_streams)
        ])
        
        # Doubly stochastic í–‰ë ¬ ìƒì„±ì„ ìœ„í•œ ë§¤ê°œë³€ìˆ˜
        # Birkhoff-von Neumann ì •ë¦¬: doubly stochastic = ìˆœì—´ í–‰ë ¬ë“¤ì˜ convex combination
        self.connection_weights = nn.Parameter(
            torch.ones(num_streams, num_streams) / num_streams
        )
    
    def _make_doubly_stochastic(self, matrix: torch.Tensor) -> torch.Tensor:
        """
        í–‰ë ¬ì„ doubly stochasticìœ¼ë¡œ ë³€í™˜
        ê° í–‰ê³¼ ì—´ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”
        """
        # Sinkhorn-Knopp ë°˜ë³µ (ê°„ë‹¨í•œ ë²„ì „)
        m = matrix.abs() + 1e-8
        
        # í–‰ ì •ê·œí™”
        m = m / m.sum(dim=1, keepdim=True)
        
        # ì—´ ì •ê·œí™”
        m = m / m.sum(dim=0, keepdim=True)
        
        return m
    
    def forward(self, *streams: torch.Tensor) -> torch.Tensor:
        """
        ì—¬ëŸ¬ residual ìŠ¤íŠ¸ë¦¼ì„ ë°›ì•„ í˜¼í•©
        
        Args:
            *streams: num_streamsê°œì˜ (batch, seq_len, hidden_size) í…ì„œ
        
        Returns:
            í˜¼í•©ëœ ì¶œë ¥ (batch, seq_len, hidden_size)
        """
        assert len(streams) == self.num_streams, \
            f"ì˜ˆìƒ {self.num_streams}ê°œ ìŠ¤íŠ¸ë¦¼, ë°›ì€ {len(streams)}ê°œ"
        
        # ê° ìŠ¤íŠ¸ë¦¼ íˆ¬ì˜
        projected_streams = [
            proj(stream) for proj, stream in zip(self.stream_projections, streams)
        ]
        
        # Doubly stochastic í˜¼í•© í–‰ë ¬
        mix_matrix = F.softmax(self.connection_weights, dim=1)
        mix_matrix = self._make_doubly_stochastic(mix_matrix)
        
        # ìŠ¤íŠ¸ë¦¼ í˜¼í•©
        # (num_streams, num_streams) @ (num_streams, ...) -> (num_streams, ...)
        mixed = torch.einsum('ij,jbsd->ibsd', mix_matrix,
                            torch.stack(projected_streams))
        
        # ìµœì¢… ì¶œë ¥ (í‰ê·  ë˜ëŠ” ê°€ì¤‘í•©)
        output = mixed.mean(dim=0)
        
        return output


class ContinuousBatcher:
    """Continuous Batching - ìš”ì²­ì„ ë™ì ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬"""
    
    def __init__(self, max_batch_size: int = 32, max_seq_len: int = 2048):
        self.max_batch_size = max_batch_size
        self.max_seq_len = max_seq_len
        self.queue = deque()
        self.active_requests = {}
    
    def add_request(self, request_id: str, tokens: List[int], max_length: int):
        """ìƒˆë¡œìš´ ìš”ì²­ ì¶”ê°€"""
        self.queue.append({
            "id": request_id,
            "tokens": tokens,
            "max_length": max_length,
            "position": len(tokens),
        })
    
    def get_batch(self) -> Optional[Dict[str, Any]]:
        """ë°°ì¹˜ ìƒì„±"""
        batch_requests = []
        total_length = 0
        
        while self.queue and len(batch_requests) < self.max_batch_size:
            req = self.queue[0]
            req_length = req["max_length"] - req["position"]
            
            if total_length + req_length <= self.max_seq_len:
                batch_requests.append(self.queue.popleft())
                total_length += req_length
            else:
                break
        
        if not batch_requests:
            return None
        
        # ë°°ì¹˜ êµ¬ì„±
        max_req_len = max(len(r["tokens"]) for r in batch_requests)
        
        batch_tokens = []
        batch_ids = []
        
        for req in batch_requests:
            tokens = req["tokens"] + [0] * (max_req_len - len(req["tokens"]))
            batch_tokens.append(tokens)
            batch_ids.append(req["id"])
        
        return {
            "tokens": torch.tensor(batch_tokens),
            "request_ids": batch_ids,
            "requests": batch_requests,
        }
    
    def update_request(self, request_id: str, new_tokens: List[int]):
        """ìš”ì²­ ì—…ë°ì´íŠ¸"""
        for req in self.queue:
            if req["id"] == request_id:
                req["tokens"] = new_tokens
                break


class TransformerLayer(nn.Module):
    """Transformer ë ˆì´ì–´ (mHC í¬í•¨)"""
    
    def __init__(self, config: AdvancedQwenConfig):
        super().__init__()
        
        # GQA ì ìš©
        if config.use_gqa:
            self.attention = GroupedQueryAttention(
                config.hidden_size, 
                config.num_attention_heads,
                num_kv_heads=config.num_kv_heads
            )
        elif config.use_flash_attention:
            self.attention = FlashAttention(
                config.hidden_size, config.num_attention_heads
            )
        else:
            self.attention = PagedAttention(
                config.hidden_size, config.num_attention_heads
            )
        
        self.mlp = nn.Sequential(
            LoRALinear(config.hidden_size, config.intermediate_size, use_lora=config.use_lora) 
            if config.use_lora else nn.Linear(config.hidden_size, config.intermediate_size),
            nn.SiLU(),
            LoRALinear(config.intermediate_size, config.hidden_size, use_lora=config.use_lora)
            if config.use_lora else nn.Linear(config.intermediate_size, config.hidden_size),
        )
        
        self.ln1 = nn.LayerNorm(config.hidden_size)
        self.ln2 = nn.LayerNorm(config.hidden_size)
        self.use_gqa = config.use_gqa
        self.use_flash_attention = config.use_flash_attention
        
        # mHC (Manifold-Constrained Hyper-Connections)
        self.use_mhc = config.use_mhc
        if config.use_mhc:
            self.mhc = ManifoldConstrainedHyperConnections(
                config.hidden_size,
                num_streams=config.mhc_num_streams
            )
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # Attention with residual
        normed = self.ln1(hidden_states)
        
        # GQA, Flash, ë˜ëŠ” Paged Attention
        if self.use_gqa or self.use_flash_attention:
            attn_out = self.attention(normed)
        else:
            attn_out, _ = self.attention(normed)
        
        # mHCë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼ ì»¤ë„¥ì…˜
        if self.use_mhc:
            # ì—¬ëŸ¬ residual ìŠ¤íŠ¸ë¦¼ ìƒì„±
            streams = [
                hidden_states,  # ì›ë³¸ ìŠ¤íŠ¸ë¦¼
                attn_out,       # Attention ìŠ¤íŠ¸ë¦¼
                hidden_states * 0.5 + attn_out * 0.5,  # í˜¼í•© ìŠ¤íŠ¸ë¦¼ 1
                hidden_states * 0.3 + attn_out * 0.7,  # í˜¼í•© ìŠ¤íŠ¸ë¦¼ 2
            ]
            hidden_states = self.mhc(*streams)
        else:
            hidden_states = hidden_states + attn_out
        
        # FFN with residual
        normed = self.ln2(hidden_states)
        mlp_out = self.mlp(normed)
        
        if self.use_mhc:
            # FFNë„ mHCë¡œ ì²˜ë¦¬
            streams = [
                hidden_states,
                mlp_out,
                hidden_states * 0.5 + mlp_out * 0.5,
                hidden_states * 0.3 + mlp_out * 0.7,
            ]
            hidden_states = self.mhc(*streams)
        else:
            hidden_states = hidden_states + mlp_out
        
        return hidden_states


class QuantizationLayer(nn.Module):
    """ê°„ë‹¨í•œ ì–‘ìí™” (INT8)"""
    
    def __init__(self, bit_width: int = 8):
        super().__init__()
        self.bit_width = bit_width
        self.scale = None
        self.zero_point = None
    
    def quantize(self, x: torch.Tensor) -> Tuple[torch.Tensor, float, int]:
        """ê°’ ì–‘ìí™”"""
        max_val = x.abs().max()
        scale = max_val / (2 ** (self.bit_width - 1) - 1)
        zero_point = 0
        
        quantized = torch.clamp(torch.round(x / scale), -2 ** (self.bit_width - 1), 2 ** (self.bit_width - 1) - 1)
        return quantized.to(torch.int8), scale, zero_point
    
    def dequantize(self, x: torch.Tensor, scale: float, zero_point: int) -> torch.Tensor:
        """ì–‘ìí™” í•´ì œ"""
        return (x.float() + zero_point) * scale


class AdvancedQwenLM(nn.Module):
    """ê³ ê¸‰ ê¸°ëŠ¥ì„ í¬í•¨í•œ LLM"""
    
    def __init__(self, config: AdvancedQwenConfig):
        super().__init__()
        self.config = config
        
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        
        # TransformerLayer ì‚¬ìš©
        self.layers = nn.ModuleList([
            TransformerLayer(config)
            for _ in range(config.num_hidden_layers)
        ])
        
        self.norm = nn.LayerNorm(config.hidden_size)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.lm_head.weight = self.embed_tokens.weight
        
        # Rope Scaling
        self.rope = RopeScaling(
            config.hidden_size // config.num_attention_heads,
            config.max_position_embeddings,
            scaling_config=config.rope_scaling,
        )
        
        # Continuous Batching
        self.batcher = ContinuousBatcher(max_seq_len=config.max_position_embeddings)
    
    def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:
        hidden_states = self.embed_tokens(input_ids)
        
        for layer in self.layers:
            hidden_states = layer(hidden_states)
        
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        
        return {"logits": logits, "hidden_states": hidden_states}


if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\n")
    
    # ê³ ê¸‰ ì„¤ì • (GQA + mHC í™œì„±í™”)
    config = AdvancedQwenConfig(
        vocab_size=50000,
        hidden_size=256,
        num_hidden_layers=4,
        num_attention_heads=8,
        intermediate_size=1024,
        use_flash_attention=True,
        use_gqa=True,
        num_kv_heads=2,
        use_lora=True,
        use_mhc=True,  # mHC í™œì„±í™”
        mhc_num_streams=4,
        rope_scaling={"type": "linear", "factor": 2.0},
    )
    
    print("=== Flash Attention í…ŒìŠ¤íŠ¸ ===")
    attn = FlashAttention(256, 8)
    hidden = torch.randn(2, 10, 256)
    output = attn(hidden)
    print(f"ì…ë ¥: {hidden.shape} â†’ ì¶œë ¥: {output.shape}\n")
    
    print("=== GQA (Grouped Query Attention) í…ŒìŠ¤íŠ¸ ===")
    gqa = GroupedQueryAttention(256, num_heads=8, num_kv_heads=2)
    hidden = torch.randn(2, 10, 256)
    output = gqa(hidden)
    print(f"GQA - ì…ë ¥: {hidden.shape} â†’ ì¶œë ¥: {output.shape}")
    print(f"  â€¢ ì¿¼ë¦¬ í—¤ë“œ: 8ê°œ")
    print(f"  â€¢ KV í—¤ë“œ: 2ê°œ")
    print(f"  â€¢ KV ìºì‹œ í¬ê¸°: 75% ê°ì†Œ!\n")
    
    print("=== mHC (Manifold-Constrained Hyper-Connections) í…ŒìŠ¤íŠ¸ ===")
    mhc = ManifoldConstrainedHyperConnections(256, num_streams=4)
    streams = [torch.randn(2, 10, 256) for _ in range(4)]
    output = mhc(*streams)
    print(f"mHC - ì…ë ¥ ìŠ¤íŠ¸ë¦¼: {len(streams)}ê°œ (ê°ê° {streams[0].shape})")
    print(f"  ì¶œë ¥: {output.shape}")
    print(f"  â€¢ Doubly stochastic í˜¼í•©ìœ¼ë¡œ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ")
    print(f"  â€¢ ìˆ˜ë ´ ì†ë„ ê°€ì†\n")
    
    print("=== LoRA í…ŒìŠ¤íŠ¸ ===")
    lora_layer = LoRALinear(256, 512, rank=8, use_lora=True)
    x = torch.randn(4, 256)
    y = lora_layer(x)
    print(f"LoRA ë ˆì´ì–´: {x.shape} â†’ {y.shape}\n")
    
    print("=== Continuous Batching í…ŒìŠ¤íŠ¸ ===")
    batcher = ContinuousBatcher(max_batch_size=4, max_seq_len=128)
    batcher.add_request("req1", [1, 2, 3], max_length=50)
    batcher.add_request("req2", [4, 5, 6, 7], max_length=60)
    batch = batcher.get_batch()
    print(f"ë°°ì¹˜ í¬ê¸°: {batch['tokens'].shape if batch else 'None'}\n")
    
    print("=== ê³ ê¸‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    model = AdvancedQwenLM(config).to(device)
    input_ids = torch.randint(0, 50000, (2, 10)).to(device)
    output = model(input_ids)
    print(f"ëª¨ë¸ ì¶œë ¥ logits shape: {output['logits'].shape}")
    
    print("\nâœ… ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ“Š ì ìš©ëœ ìµœì í™” (DeepSeek ìˆ˜ì¤€):")
    print("  â€¢ Flash Attention: 3ë°° ë¹ ë¥¸ ì¶”ë¡ ")
    print("  â€¢ GQA: KV ìºì‹œ 75% ê°ì†Œ (ë©”ëª¨ë¦¬/ì†ë„)")
    print("  â€¢ mHC: í•™ìŠµ ì•ˆì •ì„± & ìˆ˜ë ´ ì†ë„ í–¥ìƒ")
    print("  â€¢ LoRA: í•™ìŠµ íŒŒë¼ë¯¸í„° 50% ê°ì†Œ")
    print("  â€¢ Rope Scaling: 8K í† í°ê¹Œì§€ í™•ì¥")
    print("  â€¢ Continuous Batching: ì²˜ë¦¬ëŸ‰ 5ë°° ì¦ê°€")
