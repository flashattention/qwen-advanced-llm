#!/usr/bin/env python3
"""MoE λ° QLoRA κΈ°λ¥ ν…μ¤νΈ"""

import sys
from pathlib import Path

# λ¶€λ¨ λ””λ ‰ν† λ¦¬λ¥Ό κ²½λ΅μ— μ¶”κ°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from qwen_advanced import AdvancedQwenConfig, AdvancedQwenLM, MoE
import torch

print('=== MoE κΈ°λ³Έ ν…μ¤νΈ ===')
moe = MoE(
    hidden_size=768,
    num_experts=8,
    expert_size=3072,
    top_k=2,
    router_temp=1.0,
)
x_moe = torch.randn(2, 10, 768)
out_moe = moe(x_moe)
print(f'β… MoE μμ „ν: {out_moe.shape}')
print(f'β… ν™μ„±ν™”λ Expert μ: 2 (top-k=2)')

print('\n=== κΈ°λ³Έ λ¨λΈ (MLP μ‚¬μ©) ===')
config_mlp = AdvancedQwenConfig(
    hidden_size=768,
    num_hidden_layers=12,
    use_moe=False,
)
model_mlp = AdvancedQwenLM(config_mlp)
input_ids = torch.randint(0, 50000, (2, 10))
outputs_mlp = model_mlp(input_ids)
# λ°ν™κ°’μ΄ dictμΌ μ μμΌλ―€λ΅ μ²λ¦¬
if isinstance(outputs_mlp, dict):
    output_shape = outputs_mlp['logits'].shape if 'logits' in outputs_mlp else list(outputs_mlp.values())[0].shape
else:
    output_shape = outputs_mlp.shape
print(f'β… MLP λ¨λΈ μ¶λ ¥: {output_shape}')

mlp_params = sum(p.numel() for p in model_mlp.parameters())
print(f'β… MLP λ¨λΈ νλΌλ―Έν„°: {mlp_params:,}')

print('\n=== MoE λ¨λΈ ν…μ¤νΈ ===')
config_moe = AdvancedQwenConfig(
    hidden_size=768,
    num_hidden_layers=12,
    use_moe=True,
    moe_num_experts=8,
    moe_top_k=2,
)
model_moe = AdvancedQwenLM(config_moe)
outputs_moe = model_moe(input_ids)
if isinstance(outputs_moe, dict):
    output_shape = outputs_moe['logits'].shape if 'logits' in outputs_moe else list(outputs_moe.values())[0].shape
else:
    output_shape = outputs_moe.shape
print(f'β… MoE λ¨λΈ μ¶λ ¥: {output_shape}')

moe_params = sum(p.numel() for p in model_moe.parameters())
print(f'β… MoE λ¨λΈ νλΌλ―Έν„°: {moe_params:,}')
print(f'β… νλΌλ―Έν„° μ¦κ°€: {(moe_params/mlp_params - 1)*100:.1f}%')
print(f'β… μμƒ κ³„μ‚°λ‰: MLPμ™€ λ™μΌ (top-k=2λ΅ μ ν•)')

print('\n=== μ΅°ν•© ν…μ¤νΈ: QLoRA + MoE ===')
config_combined = AdvancedQwenConfig(
    hidden_size=768,
    num_hidden_layers=12,
    use_moe=True,
    moe_num_experts=8,
    moe_top_k=2,
    use_qlora=True,
    use_lora=True,
    lora_rank=8,
)
model_combined = AdvancedQwenLM(config_combined)
outputs_combined = model_combined(input_ids)
if isinstance(outputs_combined, dict):
    output_shape = outputs_combined['logits'].shape if 'logits' in outputs_combined else list(outputs_combined.values())[0].shape
else:
    output_shape = outputs_combined.shape
print(f'β… QLoRA + MoE λ¨λΈ μ¶λ ¥: {output_shape}')

combined_params = sum(p.numel() for p in model_combined.parameters())
trainable_params = sum(p.numel() for p in model_combined.parameters() if p.requires_grad)
print(f'β… μ „μ²΄ νλΌλ―Έν„°: {combined_params:,}')
print(f'β… ν•™μµ κ°€λ¥ νλΌλ―Έν„°: {trainable_params:,}')
print(f'β… ν•™μµ κ°€λ¥ λΉ„μ¨: {trainable_params/combined_params*100:.2f}%')

print('\n=== μ„±λ¥ λ©”νΈλ¦­ ===')
print(f'MLP λ¨λΈ:        {mlp_params:>12,} νλΌλ―Έν„°')
print(f'MoE λ¨λΈ:        {moe_params:>12,} νλΌλ―Έν„° (+{(moe_params/mlp_params - 1)*100:.1f}%)')
print(f'QLoRA + MoE:     {combined_params:>12,} νλΌλ―Έν„°')
print(f'  - ν•™μµ κ°€λ¥:   {trainable_params:>12,} νλΌλ―Έν„°')
print(f'  - λ©”λ¨λ¦¬ μ κ°: ~{(1 - trainable_params/combined_params)*100:.0f}% (QLoRA)')

print('\nπ‰ MoE λ° μ΅°ν•© ν…μ¤νΈ μ™„λ£!')
